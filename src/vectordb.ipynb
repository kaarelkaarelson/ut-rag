{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "import openai\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "\n",
    "# Settings.embed_model = HuggingFaceEmbedding(\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# )\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model = 'text-embedding-3-small')\n",
    "\n",
    "embedding_function = OpenAIEmbeddingFunction(model_name = 'text-embedding-3-small', api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "Settings.llm = OpenAI(model = 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "# chroma_client = chromadb.HttpClient(host=\"chroma\", port = 8000, settings=Settings(allow_reset=True, anonymized_telemetry=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_docs_to_collection(folder_name, collection):\n",
    "    documents_objects = SimpleDirectoryReader(input_dir=f\"../docs/{folder_name}\").load_data()\n",
    "    \n",
    "    documents = [document.text for document in documents_objects] # ids have to be unique identifiers for the documents, here we choose the file name for simplicity.\n",
    "    ids = [document.metadata['file_name'] for document in documents_objects] # ids have to be unique identifiers for the documents, here we choose the file name for simplicity.\n",
    "\n",
    "    collection.add(documents=documents,\n",
    "                   ids=ids)\n",
    "\n",
    "    print(f\"Folder {folder_name} successfully added to the collection\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    collection = client.get_collection(name=\"docs_collection\")\n",
    "    collection_exists = True\n",
    "except ValueError:\n",
    "    collection_exists = False\n",
    "\n",
    "if not collection_exists:\n",
    "    print('COLLECTION DOES NOT EXIST')\n",
    "    collection = client.create_collection(name=\"docs_collection\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    dir_as_list = os.listdir(\"../docs\")\n",
    "    print(dir_as_list)\n",
    "\n",
    "    for dir in dir_as_list:\n",
    "        add_docs_to_collection(dir, collection)\n",
    "else:\n",
    "    print('COLLECTION EXISTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"mitme täiendkoolituse vahel on võimalik valida?\"], # Chroma will embed this for you\n",
    "    n_results=4 # how many results to return\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=collection) \n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"What does the Autonomous Driving Lab help to lay foundations for?\"\n",
    "q2 = \"Where does Applied Cyber Security Group get it's funding from?\"\n",
    "q3 = \"What room does autonomous driving lab work in?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)   \n",
    "\n",
    "res = chat_engine.chat(\"Kui palju tudengeid on Tartu ülikoolis?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index.as_query_engine(llm=\"Mis teleskoobid on Tartu ülikoolil?\")\n",
    "\n",
    "res = index.as_retriever().retrieve(\"What does the Autonomous Driving Lab help to lay foundations for?\")\n",
    "print(res)\n",
    "docs_names = [res_obj.node.node_id for res_obj in res]\n",
    "print(docs_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    collection = client.get_collection(name=\"docs_openai2_collection\", embedding_function=embedding_function)\n",
    "    collection_exists = True\n",
    "except ValueError:\n",
    "    collection_exists = False\n",
    "\n",
    "if not collection_exists:\n",
    "    print('COLLECTION DOES NOT EXIST')\n",
    "    collection = client.create_collection(name=\"docs_openai2_collection\", embedding_function=embedding_function, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    documents = SimpleDirectoryReader(input_dir=\"../docs\", recursive=True).load_data()\n",
    "\n",
    "    # set up ChromaVectorStore and load in data\n",
    "    vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents, storage_context=storage_context\n",
    "    )\n",
    "else:\n",
    "    print('COLLECTION EXISTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(name=\"docs_openai2_collection\", embedding_function=embedding_function)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat_engine.chat(\"Kui palju tudengeid on Tartu ülikoolis?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_as_list = os.listdir(\"../docs\")\n",
    "print(dir_as_list)\n",
    "\n",
    "for dir in dir_as_list:\n",
    "    if dir == 'ut_ee':\n",
    "        continue\n",
    "    add_docs_to_collection(dir, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name=\"quickstart\",\n",
    "    dimension=1536,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset of Docs Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents_objects = SimpleDirectoryReader(input_dir=\"../docs\", recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def chunk_text(text, max_tokens):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_len = 0\n",
    "\n",
    "    # Iterate through the tokens and create chunks\n",
    "    for token in tokens:\n",
    "        token_len = len(token)\n",
    "        if current_chunk_len + token_len + 1 > max_tokens:  # +1 for the space\n",
    "            # If adding the next token exceeds the limit, save the current chunk\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [token]\n",
    "            current_chunk_len = token_len\n",
    "        else:\n",
    "            # Add the token to the current chunk\n",
    "            current_chunk.append(token)\n",
    "            current_chunk_len += token_len + 1  # +1 for the space\n",
    "\n",
    "    # Append the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re \n",
    "\n",
    "def get_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    link_tag = re.findall(r\"(<LINK>.*?<\\/LINK>)\", text)[0]\n",
    "    num_tokens = num_tokens_from_string(text)\n",
    "\n",
    "    if num_tokens > 8191:\n",
    "        print(f\"Tokens limit exceede, tokens: {num_tokens}\")\n",
    "        text_chunks = chunk_text(text, 8191)\n",
    "        text_chunks = [link_tag + chunk for chunk in text_chunks]\n",
    "        embeddings = openai.embeddings.create(input = text_chunks, model=model)\n",
    "       \n",
    "        return [data.embedding for data in embeddings.data], text_chunks\n",
    "    else:\n",
    "        return [openai.embeddings.create(input = [text], model=model).data[0].embedding], [text]\n",
    "    \n",
    "\n",
    "def divide_text_into_chunks(text, token_limit):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    if num_tokens > token_limit:\n",
    "        num_chunks = math.ceil(num_tokens / token_limit)\n",
    "        num_symbols_per_chunk = math.floor(len(text) / num_chunks)\n",
    "        text_chunks = []\n",
    "        for i in range(0, len(text), num_symbols_per_chunk):\n",
    "            text_chunk = text[i: i+num_symbols_per_chunk]\n",
    "            text_chunks.append(text_chunk)\n",
    "        return text_chunks\n",
    "    else:\n",
    "        return [text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "docs_df = pd.DataFrame({ \n",
    "    'id': [document.metadata['file_name'] for document in documents_objects], \n",
    "    'values': [''] * len(documents_objects),\n",
    "    'metadata': [{'text': document.text} for document in documents_objects]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "docs_add_df = pd.DataFrame(columns=[\"id\", \"values\", \"metadata\"])\n",
    "\n",
    "for index, row in docs_df.iterrows():\n",
    "    id = row['id']\n",
    "    text = row['metadata']['text']\n",
    "    num_tokens = num_tokens_from_string(text)\n",
    "\n",
    "    embeddings, texts = get_embeddings(text)\n",
    "\n",
    "    if len(embeddings) > 1: \n",
    "        print(len(embeddings), len(texts))\n",
    "        print()\n",
    "        for i, (embedding, text) in enumerate(zip(embeddings, texts)):\n",
    "            docs_add_df.loc[len(docs_add_df)] = [f\"{id}-{i+1}\", embedding, {'text': text}]\n",
    "        docs_df.drop(index, inplace=True)\n",
    "    else: \n",
    "        row['values'] = embeddings[0]\n",
    "\n",
    "    print(f\"{index}: id: {id}, num_of_embeddings: {len(embeddings)}\")\n",
    "    count += 1\n",
    "    if count % 500 == 0:\n",
    "        docs_df.to_csv('docs_df_local.csv', index=False)\n",
    "\n",
    "docs_df = pd.concat([docs_df, docs_add_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_add_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "docs_df['metadata'] = docs_df['metadata'].apply(json.dumps)\n",
    "docs_df['values'] = docs_df['values'].apply(json.dumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV\n",
    "docs_df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DataFrame from CSV\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'embedding': 'values'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['values'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Decode JSON strings back into JSON objects\n",
    "df['metadata'] = df['metadata'].apply(json.loads)\n",
    "df['values'] = df['values'].apply(lambda x: [float(i) for i in ast.literal_eval(x)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metadata'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_chunks(iterable, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "\n",
    "    print(text_chunks)\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize without metadata filter\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    raise EnvironmentError(f\"Environment variable OPENAI_API_KEY is not set\")\n",
    "\n",
    "vector_store = PineconeVectorStore(pinecone_index=index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(iterable, batch_size=100):\n",
    "    \"\"\"A helper function to break an iterable into chunks of size batch_size.\"\"\"\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"id\": row['id'], \"values\": row['values'], \"metadata\": row['metadata']} for _, row in df.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data[0]['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert data with 100 vectors per upsert request\n",
    "for ids_vectors_chunk in chunks(data, batch_size=100):\n",
    "    print(ids_vectors_chunk)\n",
    "    index.upsert(vectors=ids_vectors_chunk) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "import itertools\n",
    "\n",
    "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "index = pc.Index(\"quickstart\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize without metadata filter\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    raise EnvironmentError(f\"Environment variable OPENAI_API_KEY is not set\")\n",
    "\n",
    "vector_store = PineconeVectorStore(pinecone_index=index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Üle 160 õppekava\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Kui palju õppekavasid on Tartu ülikoolis\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB! DO THIS AT YOUR OWN RISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_docs_to_collection(\"ut_ee\", collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(\"docs_openai2_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ut_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
